# Intuitive Care - Desafio T√©cnico (Est√°gio)

Este reposit√≥rio cont√©m a solu√ß√£o Full-Stack para o desafio t√©cnico da Intuitive Care. O projeto foi estruturado como um **Monorepo** que abrange todo o ciclo de vida dos dados: Engenharia de Dados (ETL), Enriquecimento (Data Enrichment), Banco de Dados (SQL) e Desenvolvimento Web (Vue.js + Python).

## üìÇ Estrutura do Projeto

O projeto foi organizado em m√≥dulos independentes que funcionam como um Pipeline de Dados sequencial:

* **`1_etl_ans/`** **(Tarefa 1)**
    * *Fun√ß√£o:* Extra√ß√£o (`Extract`).
    * *Descri√ß√£o:* Scripts respons√°veis por varrer o site da ANS via scraping, baixar e consolidar os dados brutos cont√°beis.
* **`2_transformacao/`** **(Tarefa 2)**
    * *Fun√ß√£o:* Transforma√ß√£o (`Transform`).
    * *Descri√ß√£o:* Scripts que enriquecem os dados cruzando com o cadastro oficial (CADOP), validam regras de neg√≥cio, geram as agrega√ß√µes estat√≠sticas e **persistem dados intermedi√°rios**.
* **`3_banco_dados/`** **(Tarefa 3)**
    * *Fun√ß√£o:* Modelagem e Carga (`Load/Storage`).
    * *Descri√ß√£o:* Orquestrador em Python e scripts SQL para modelagem do banco de dados relacional (PostgreSQL) e execu√ß√£o de queries anal√≠ticas.
* **`4_interface_web/`** **(Tarefa 4)**
    * *Fun√ß√£o:* Visualiza√ß√£o (`Frontend`).
    * *Descri√ß√£o:* Aplica√ß√£o Web (Frontend Vue.js + Backend Python) para visualiza√ß√£o dos dados processados.

---

## ‚öôÔ∏è Pr√©-requisitos

* **Python 3.10+**
* **PostgreSQL 14+** (Rodando localmente na porta 5432)
* **Gerenciador de pacotes:** `pip`

---

## üöÄ Como Executar o Pipeline de Dados

Para garantir a integridade e rastreabilidade dos dados, a execu√ß√£o deve seguir a ordem abaixo:

### Passo 1: Extra√ß√£o de Dados Brutos (ETL)

Este script conecta-se ao servidor FTP da ANS, identifica os 3 trimestres mais recentes, baixa os arquivos ZIP (lidando com estruturas de pastas variadas) e consolida tudo em um √∫nico CSV.

```bash
cd 1_etl_ans
# Crie e ative seu ambiente virtual, se necess√°rio
pip install -r requirements.txt
python main.py
```

- **Sa√≠da:** `output/consolidado_despesas.csv`

- **Nota:** O arquivo gerado mant√©m a coluna **RegistroANS** como chave prim√°ria.  
  As colunas **CNPJ** e **Raz√£o Social** s√£o preenchidas com `"N/A"`, pois os arquivos cont√°beis originais n√£o disponibilizam essas informa√ß√µes.

---

### Passo 2: Transforma√ß√£o, Enriquecimento e Valida√ß√£o

Nesta etapa, o script l√™ o arquivo bruto, baixa o **Cadastro de Operadoras (CADOP)**, realiza o cruzamento de dados e gera arquivos para an√°lise.

**Atualiza√ß√£o:** O script agora salva uma c√≥pia do CADOP bruto (`relatorio_cadop.csv`) para ser consumido posteriormente pelo Banco de Dados, evitando necessidade de novo scraping.

```bash
# Partindo da pasta anterior (1_etl_ans)
cd ../2_transformacao
pip install -r requirements.txt
python main.py
```

### Sa√≠das Geradas

1. `output/despesas_agregadas.csv`  
   *(Dados processados e somados por UF).*

2. `output/Teste_JoaoGabriel.zip`  
   *(Arquivo final compactado).*

3. `output/relatorio_cadop.csv`  
   **(Novo:** Arquivo bruto para carga no Banco de Dados).*

---

### Passo 3: Banco de Dados e An√°lise SQL

Esta etapa carrega os dados processados em um banco **PostgreSQL**.  
Foi desenvolvido um orquestrador em Python que resolve problemas de permiss√£o de arquivos no Linux (copiando temporariamente para `/tmp`) e injeta os caminhos absolutos corretos nos scripts SQL.

```bash
cd ../3_banco_dados
pip install -r requirements.txt
python main.py
```

O script solicitar√° seu usu√°rio e senha do PostgreSQL local, criar√° o banco intuitive_care_db e executar√° a carga automaticamente.
Para verificar os resultados das queries anal√≠ticas via terminal:

```bash
psql -h localhost -U postgres -d intuitive_care_db -f 3_queries_analiticas.sql
```

## üß† Trade-offs e Decis√µes T√©cnicas (Documenta√ß√£o Obrigat√≥ria)

Abaixo est√£o as justificativas para as abordagens t√©cnicas adotadas em cada etapa do desafio.

---

#### üìã Tarefa 1: Extra√ß√£o (ETL)

### Scraping Din√¢mico vs URL Est√°tica

- **Decis√£o:** Scraping din√¢mico com **BeautifulSoup**.

- **Justificativa:**  
  As URLs no site da ANS mudam frequentemente (troca de ano ou vers√£o do arquivo).  
  O script varre automaticamente a estrutura de pastas do FTP para localizar o dado mais recente, tornando a solu√ß√£o **resiliente a mudan√ßas estruturais** e reduzindo manuten√ß√£o manual.

---

#### üìã Tarefa 2: Transforma√ß√£o e Enriquecimento

### 1. Estrat√©gia de Join e Integridade

- **Decis√£o:** Utilizar o **RegistroANS** como chave (Foreign Key) em vez do CNPJ.

  - **Justificativa:**  
    Os arquivos cont√°beis brutos n√£o cont√™m CNPJ.  
    O **RegistroANS** √© o identificador √∫nico e imut√°vel definido pela ag√™ncia reguladora, garantindo integridade referencial.

- **Decis√£o:** Utiliza√ß√£o de `pandas.merge` (**Hash Join em mem√≥ria**).

  - **Justificativa:**  
    O volume total de dados, somado ao cadastro (‚âà 1.200 registros), cabe confortavelmente na RAM (< 1 GB).  
    O processamento em mem√≥ria √© **ordens de magnitude mais r√°pido** do que o uso de bancos intermedi√°rios ou escrita em disco.

---

### 2. Valida√ß√£o e Tratamento de Dados

- **CNPJs inv√°lidos:** Mantidos, com gera√ß√£o de logs de aviso.

  - **Justificativa:**  
    A exclus√£o desses registros mascararia o volume financeiro real do setor.  
    Priorizou-se a **fidelidade cont√°bil** em detrimento da pureza cadastral.

- **Valores zerados:** Removidos.

  - **Justificativa:**  
    Esses registros distorcem m√©tricas estat√≠sticas como m√©dia, soma e desvio padr√£o, impactando an√°lises anal√≠ticas.

---

#### üìã Tarefa 3: Banco de Dados (SQL)

### 1. Modelagem: Normaliza√ß√£o vs Desnormaliza√ß√£o

- **Escolha:** Abordagem h√≠brida (**Star Schema**).

  - **Tabela Dimens√£o (`operadoras`):**  
    Armazena dados cadastrais est√°veis das operadoras.

  - **Tabela Fato (`despesas_contabeis`):**  
    Armazena eventos financeiros, referenciando a operadora por ID.

  - **Tabela Agregada (`despesas_agregadas_final`):**  
    Estrutura desnormalizada para leitura e an√°lise r√°pida.

- **Justificativa:**  
  As despesas crescem exponencialmente ao longo do tempo, enquanto os dados cadastrais permanecem majoritariamente est√°veis.  
  Repetir strings como **Raz√£o Social** na tabela de fatos aumentaria uso de armazenamento e I/O.  
  A normaliza√ß√£o otimiza atualiza√ß√µes cadastrais, enquanto a tabela agregada acelera consultas anal√≠ticas.

---

### 2. Tipos de Dados (Data Types)

- **Valores Monet√°rios:** `DECIMAL(15, 2)` vs `FLOAT`.

  - **Decis√£o:** `DECIMAL(15, 2)`.

  - **Justificativa:**  
    Tipos `FLOAT` utilizam ponto flutuante bin√°rio, introduzindo erros de arredondamento em opera√ß√µes financeiras.  
    `DECIMAL` garante **precis√£o exata**, essencial para dados cont√°beis.

- **Datas:** `DATE` vs `VARCHAR`.

  - **Decis√£o:** `DATE`.

  - **Justificativa:**  
    `VARCHAR` impede ordena√ß√£o cronol√≥gica correta e dificulta indexa√ß√£o.  
    O trimestre foi convertido para `DATE` (dia 01 do m√™s inicial do trimestre), facilitando s√©ries temporais, filtros e √≠ndices.

---

### 3. Tratamento de Inconsist√™ncias na Importa√ß√£o

- **Encoding:**  
  Convers√£o expl√≠cita de **LATIN1** (CADOP) e **UTF-8** (Despesas) nos comandos `COPY`, evitando corrup√ß√£o de caracteres.

- **Limpeza de Strings:**  
  Uso de `REGEXP_REPLACE` no SQL para sanitizar o campo **RegistroANS** antes da convers√£o para inteiro.

- **Truncagem de UF:**  
  Tratamento de registros onde a UF vinha como `"N/A"` (3 caracteres) para uma coluna `CHAR(2)`.  
  Aplicou-se `LEFT(uf, 2)` combinado com `NULLIF`, garantindo que o pipeline n√£o falhasse.

---

## üõ† Tecnologias Utilizadas

### üî§ Linguagem e Bibliotecas

- **Python 3.10+**
- **pandas:** Manipula√ß√£o de dados e agrega√ß√µes.
- **requests / BeautifulSoup:** Scraping e download de arquivos.
- **psycopg2 / SQLAlchemy:** Conectividade com banco de dados.

---

### üóÑÔ∏è Banco de Dados

- **PostgreSQL 14+**  
  Banco de dados relacional utilizado para armazenamento, modelagem e an√°lise anal√≠tica.
