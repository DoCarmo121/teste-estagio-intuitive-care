# Intuitive Care - Desafio TÃ©cnico (EstÃ¡gio)

Este repositÃ³rio contÃ©m a soluÃ§Ã£o Full-Stack para o desafio tÃ©cnico da Intuitive Care. O projeto foi estruturado como um **Monorepo** que abrange todo o ciclo de vida dos dados: Engenharia de Dados (ETL), Enriquecimento (Data Enrichment), Banco de Dados (SQL) e Desenvolvimento Web (Vue.js + Python).

## ğŸ“‚ Estrutura do Projeto

O projeto foi organizado em mÃ³dulos independentes que funcionam como um Pipeline de Dados sequencial:

* **`1_etl_ans/`** **(Tarefa 1)**
    * *FunÃ§Ã£o:* ExtraÃ§Ã£o (`Extract`).
    * *DescriÃ§Ã£o:* Scripts responsÃ¡veis por varrer o site da ANS via scraping, baixar e consolidar os dados brutos contÃ¡beis.
* **`2_transformacao/`** **(Tarefa 2)**
    * *FunÃ§Ã£o:* TransformaÃ§Ã£o (`Transform`).
    * *DescriÃ§Ã£o:* Scripts que enriquecem os dados cruzando com o cadastro oficial (CADOP), validam regras de negÃ³cio, geram as agregaÃ§Ãµes estatÃ­sticas e **persistem dados intermediÃ¡rios**.
* **`3_banco_dados/`** **(Tarefa 3)**
    * *FunÃ§Ã£o:* Modelagem e Carga (`Load/Storage`).
    * *DescriÃ§Ã£o:* Orquestrador em Python e scripts SQL para modelagem do banco de dados relacional (PostgreSQL) e execuÃ§Ã£o de queries analÃ­ticas.
* **`4_interface_web/`** **(Tarefa 4)**
    * *FunÃ§Ã£o:* VisualizaÃ§Ã£o (`Frontend`).
    * *DescriÃ§Ã£o:* AplicaÃ§Ã£o Web (Frontend Vue.js + Backend Python) para visualizaÃ§Ã£o dos dados processados.

---

## âš™ï¸ PrÃ©-requisitos

* **Python 3.10+**
* **PostgreSQL 14+** (Rodando localmente na porta 5432)
* **Gerenciador de pacotes:** `pip`

---

## ğŸš€ Como Executar o Pipeline de Dados

Para garantir a integridade e rastreabilidade dos dados, a execuÃ§Ã£o deve seguir a ordem abaixo:

### Passo 1: ExtraÃ§Ã£o de Dados Brutos (ETL)

Este script conecta-se ao servidor FTP da ANS, identifica os 3 trimestres mais recentes, baixa os arquivos ZIP (lidando com estruturas de pastas variadas) e consolida tudo em um Ãºnico CSV.

```bash
cd 1_etl_ans
# Crie e ative seu ambiente virtual, se necessÃ¡rio
pip install -r requirements.txt
python main.py
```

- **SaÃ­da:** `output/consolidado_despesas.csv`

- **Nota:** O arquivo gerado mantÃ©m a coluna **RegistroANS** como chave primÃ¡ria.  
  As colunas **CNPJ** e **RazÃ£o Social** sÃ£o preenchidas com `"N/A"`, pois os arquivos contÃ¡beis originais nÃ£o disponibilizam essas informaÃ§Ãµes.

---

### Passo 2: TransformaÃ§Ã£o, Enriquecimento e ValidaÃ§Ã£o

Nesta etapa, o script lÃª o arquivo bruto, baixa o **Cadastro de Operadoras (CADOP)**, realiza o cruzamento de dados e gera arquivos para anÃ¡lise.

**AtualizaÃ§Ã£o:** O script agora salva uma cÃ³pia do CADOP bruto (`relatorio_cadop.csv`) para ser consumido posteriormente pelo Banco de Dados, evitando necessidade de novo scraping.

```bash
# Partindo da pasta anterior (1_etl_ans)
cd ../2_transformacao
pip install -r requirements.txt
python main.py
```

### SaÃ­das Geradas

1. `output/despesas_agregadas.csv`  
   *(Dados processados e somados por UF).*

2. `output/Teste_JoaoGabriel.zip`  
   *(Arquivo final compactado).*

3. `output/relatorio_cadop.csv`  
   **(Novo:** Arquivo bruto para carga no Banco de Dados).*

---

### Passo 3: Banco de Dados e AnÃ¡lise SQL

Esta etapa carrega os dados processados em um banco **PostgreSQL**.  
Foi desenvolvido um orquestrador em Python que resolve problemas de permissÃ£o de arquivos no Linux (copiando temporariamente para `/tmp`) e injeta os caminhos absolutos corretos nos scripts SQL.

```bash
cd ../3_banco_dados
pip install -r requirements.txt
python main.py
```

O script solicitarÃ¡ seu usuÃ¡rio e senha do PostgreSQL local, criarÃ¡ o banco intuitive_care_db e executarÃ¡ a carga automaticamente.
Para verificar os resultados das queries analÃ­ticas via terminal:

```bash
psql -h localhost -U postgres -d intuitive_care_db -f 3_queries_analiticas.sql
```

# ğŸ§  Trade-offs e DecisÃµes TÃ©cnicas  
**DocumentaÃ§Ã£o ObrigatÃ³ria**

Este documento descreve as principais decisÃµes tÃ©cnicas adotadas no pipeline de dados, destacando os trade-offs entre performance, qualidade de dados, simplicidade e escalabilidade.

---

## 1. Processamento e ExtraÃ§Ã£o (ETL)

### âš¡ Processamento: MemÃ³ria vs. Incremental vs. Stream

**DecisÃ£o:** Abordagem hÃ­brida â€” *Download via Stream* + *Processamento In-Memory*.

**Justificativa:**

- **Download:**  
  Os arquivos ZIP sÃ£o baixados em *chunks* de 8 KB, reduzindo picos de memÃ³ria e tornando o processo mais resiliente a falhas de rede ou arquivos inesperadamente grandes.

- **Processamento:**  
  O volume consolidado dos trÃªs trimestres, mesmo apÃ³s descompactaÃ§Ã£o, permanece abaixo de 2 GB, cabendo confortavelmente na memÃ³ria RAM.  
  O uso de operaÃ§Ãµes vetorizadas do **Pandas (In-Memory)** Ã© ordens de magnitude mais rÃ¡pido do que abordagens baseadas em disco ou frameworks distribuÃ­dos (ex: Spark), que seriam excessivos para esse cenÃ¡rio.

---

### ğŸ“… InconsistÃªncia de Datas

**Problema:**  
A coluna de data nos CSVs originais apresenta mÃºltiplos formatos inconsistentes (`1T2024`, `01/01/2024`, `jan/24`).

**DecisÃ£o:**  
Ignorar a data interna dos arquivos.

**SoluÃ§Ã£o:**  
Utilizar a estrutura de diretÃ³rios do FTP da ANS como *Source of Truth*, injetando programaticamente as colunas **Ano** e **Trimestre**.

**BenefÃ­cio:**  
Elimina ambiguidades e garante consistÃªncia temporal 100% confiÃ¡vel.

---

## 2. TransformaÃ§Ã£o e Enriquecimento

### ğŸ”— EstratÃ©gia de Join e Integridade (RegistroANS)

**DecisÃ£o:**  
Utilizar `RegistroANS` como chave primÃ¡ria de ligaÃ§Ã£o, com `pandas.merge` (Hash Join).

**Problema:**  
Os arquivos contÃ¡beis nÃ£o possuem CNPJ, apenas o identificador `REG_ANS`.

**SoluÃ§Ã£o:**  

- **Tarefa 1:** ExtraÃ§Ã£o fiel dos dados contÃ¡beis.  
- **Tarefa 2:** Camada de *Trusted Data*, com download do CADOP oficial e *Left Join* via `RegistroANS`.

**BenefÃ­cios:**

- Garante integridade referencial sem depender de dados inexistentes na fonte.
- Hash Join em memÃ³ria apresenta complexidade **O(N)**, ideal para datasets deste porte.

---

### ğŸ§¾ Tratamento de CNPJs InvÃ¡lidos

**Trade-off:** Fidelidade contÃ¡bil vs. pureza cadastral.

**DecisÃ£o:**  
Manter os registros, mas gerar *log de auditoria*.

**Justificativa:**

- **PrÃ³s:**  
  O volume financeiro agregado do setor permanece correto. Remover linhas distorceria o balanÃ§o contÃ¡bil.
- **Contras:**  
  O dataset final contÃ©m dados cadastrais inconsistentes, que devem ser tratados na camada de visualizaÃ§Ã£o ou consumo.

---

### ğŸ”¢ Tratamento de Valores Zerados ou Negativos

**DecisÃ£o:**  
Filtragem rigorosa â€” `valor > 0`.

**Justificativa:**  
Valores negativos (estornos) ou nulos distorcem mÃ©tricas estatÃ­sticas como **mÃ©dia** e **desvio padrÃ£o**, que sÃ£o centrais para a anÃ¡lise solicitada.  
A remoÃ§Ã£o garante relevÃ¢ncia estatÃ­stica e coerÃªncia analÃ­tica.

---

### ğŸ“‰ EstratÃ©gia de OrdenaÃ§Ã£o

**DecisÃ£o:**  
OrdenaÃ§Ã£o em memÃ³ria com `df.sort_values` (Quicksort interno).

**Justificativa:**  
ApÃ³s a agregaÃ§Ã£o (`GROUP BY`), o volume de dados reduz-se drasticamente (para poucos milhares de linhas).  
O custo computacional da ordenaÃ§Ã£o em memÃ³ria torna-se desprezÃ­vel, nÃ£o justificando *external sort* ou uso de banco de dados apenas para essa etapa.

---

## 3. Banco de Dados (SQL)

### ğŸ—ï¸ Modelagem: NormalizaÃ§Ã£o â€” OpÃ§Ã£o A vs. OpÃ§Ã£o B

**DecisÃ£o:**  
**OpÃ§Ã£o B â€” Modelo Normalizado (Star Schema)**

- **Tabela DimensÃ£o:** `operadoras` (dados cadastrais)
- **Tabela Fato:** `despesas_contabeis` (eventos financeiros)

**Justificativa:**

- **Volume:**  
  As despesas crescem exponencialmente a cada trimestre, enquanto os dados cadastrais sÃ£o estÃ¡veis.
- **EficiÃªncia:**  
  Evita repetiÃ§Ã£o massiva de strings como *RazÃ£o Social*, reduzindo armazenamento e I/O.
- **Manutenibilidade:**  
  AtualizaÃ§Ãµes cadastrais exigem alteraÃ§Ã£o em apenas uma linha, garantindo consistÃªncia (ACID).

---

### ğŸ’² Tipos de Dados: DECIMAL vs. FLOAT

**DecisÃ£o:**  
`DECIMAL(15,2)`

**Justificativa:**  
Tipos `FLOAT` utilizam ponto flutuante binÃ¡rio (IEEE 754), introduzindo erros de precisÃ£o (`0.1 + 0.2 â‰  0.3`).  
Para dados financeiros, **precisÃ£o exata Ã© obrigatÃ³ria**, tornando `DECIMAL` a escolha correta.

---

### ğŸ—“ï¸ Tipos de Dados: DATE vs. VARCHAR

**DecisÃ£o:**  
`DATE`

**Justificativa:**  

- Permite ordenaÃ§Ã£o cronolÃ³gica correta.
- Viabiliza uso eficiente de funÃ§Ãµes de data, indexaÃ§Ã£o e particionamento.
- O trimestre foi convertido para o primeiro dia do mÃªs correspondente  
  *(ex: 1Âº trimestre â†’ `2023-01-01`)*.

---

## 4. Queries AnalÃ­ticas

### ğŸ§  Operadoras Acima da MÃ©dia em 2 ou Mais Trimestres

**DecisÃ£o:**  
Uso de **CTEs (Common Table Expressions)** + agregaÃ§Ã£o com `HAVING`.

**EstratÃ©gia:**

1. **CTE `media_trimestral`:**  
   Calcula a mÃ©dia global de despesas por trimestre.
2. **CTE `performance`:**  
   Compara cada operadora com a mÃ©dia do trimestre (flag 0 ou 1).
3. **Query final:**  
   Soma os flags e filtra operadoras com `SUM >= 2`.

**Justificativa:**

- **Legibilidade:**  
  CTEs tornam a query linear, clara e autodocumentÃ¡vel.
- **Performance:**  
  O *Query Planner* do PostgreSQL consegue materializar as CTEs de forma eficiente, evitando recÃ¡lculos redundantes da mÃ©dia global.

---

## ğŸ›  Tecnologias Utilizadas

### ğŸ”¤ Linguagem e Bibliotecas

- **Python 3.10+**
- **pandas:** ManipulaÃ§Ã£o de dados e agregaÃ§Ãµes.
- **requests / BeautifulSoup:** Scraping e download de arquivos.
- **psycopg2 / SQLAlchemy:** Conectividade com banco de dados.

---

### ğŸ—„ï¸ Banco de Dados

- **PostgreSQL 14+**  
  Banco de dados relacional utilizado para armazenamento, modelagem e anÃ¡lise analÃ­tica.
